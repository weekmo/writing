\documentclass[../structure.tex]{subfiles}
%\usepackage{../mypkg}
\begin{document}
\chapter{Methods and Implementation}
\hspace{2em}Before we discuss the ICP method and its implementation, let us first briefly review the principle of affine transformation. If we have a point $[x,y,1]$ in homogeneous coordinate in 2D euclidean space and we want to move this point three units through the \textit{X axis}, $[x,y,1]^T a = [x+3,y,1]^T$, where $a$ will be  denoted as the affine matrix.

\begin{equation*}
\begin{bmatrix}
x \\ y \\ 1
\end{bmatrix}
\begin{bmatrix}
1 & 0 & 3 \\
0 & 1 & 0 \\
0 & 0 & 1
\end{bmatrix}
=
\begin{bmatrix}
x + 3 \\ y \\ 1
\end{bmatrix}
\end{equation*}

The example mentioned above is only for translation; we still have some other moves (i.e. sheer, rotation and scale) which can be performed, as demonstrated in figure \{\ref{fig:affine}\} for 2D shape.

\begin{figure}[h!]
\centering
\includegraphics[scale=0.5]{007_affine}
\captionsetup{justification=centering}
\caption{2D affine transformation matrices \cite{Wikipedia2016}}
\label{fig:affine}
\end{figure}

We have shown how 2D transformation occurs; the same principles apply to 3D as well, as demonstrated below:

\begin{equation*}
\begin{bmatrix}
\hat{x} \\ \hat{y} \\ \hat{z} \\ 1
\end{bmatrix}
\begin{bmatrix}
a & b & c & e\\
f & g & h & i\\
j & k & l & m\\
0 & 0 & 0 & 1
\end{bmatrix}
=
\begin{bmatrix}
x \\ y \\ z \\ 1
\end{bmatrix}
\end{equation*}

However,  this is still in the case of one point; if we have more points, as in our case, the equation above becomes:

\begin{equation*}
\begin{bmatrix}
\hat{x_{1}} & \hat{x_{2}} & \dots & \hat{x_{n}}\\
\hat{y_{1}} & \hat{y_{2}} & \dots & \hat{y_{n}}\\
\hat{z_{1}} & \hat{z_{2}} & \dots & \hat{z_{n}}\\
1 & 1 & \dots & 1
\end{bmatrix}
\begin{bmatrix}
a & b & c & e\\
f & g & h & i\\
j & k & l & m\\
0 & 0 & 0 & 1
\end{bmatrix}
=
\begin{bmatrix}
x_{1} & x_{2} & \dots & x_{n}\\
y_{1} & y_{2} & \dots & y_{n}\\
z_{1} & z_{2} & \dots & z_{n}\\
1 & 1 & \dots & 1
\end{bmatrix}
\end{equation*}

\hspace{2em}As we are implementing the registration method, we are going to consider two objects, target graph $T$, which remains fixed, and template graph $S$, which moves iteratively until it reaches the optimal alignment with target graph $T$. To do so, we use Iterative Closest Point (ICP) as previously discussed (see figure \{\ref{fig:icp}\}). With ICP, for each point on the template graph $S$, we look for the closest point on the target graph $S$. Then we start moving each point in $S$ to a correspondent point in $T$ with respect to stiffness. After solving the cost function by using \textit{LSQR} we have a separate \textit{Affine Transformation Matrix} ($X = [X_{1}, X_{2}, X_{3}, ...,X_{n}]$) for each point in the template graph $S$ that moves separately whilst keeping the original neighbors as close to each other as possible. This type of registration is called non-rigid registration.

\begin{figure}[h!]
\centering
\includegraphics[scale=0.5]{001_conn}
\captionsetup{justification=centering}
\caption{The template graph $S$ (green) is deformed by locally applying affine transformations $(X_{i})$ onto the target graph $T$ (red). The algorithm determines the closest points $(u_{i})$ for each displaced source vertex $(X_{i}v_{i})$ and finds the optimal deformation for the stiffness used in this iteration. This is repeated until a stable state is found. The process then continues with a lower stiffness. Due to the stiffness constraint, the vertices do not move directly towards the target graph, but may move parallel along it. The correspondences $u_{1}$
and $u_{4}$ are dropped as they lie on the border of the target \cite{Amberg2007}.}
\label{fig:icp}
\end{figure}

We implement the method in \cite{Amberg2007} by using the Python programming language, Spyder as an IDE and several packages, namely PlyFile, DIPY, NiBabel, Open3D, Numpy, Scipy, Scikit-learn and Matplotlib.

\section{Data preparation}
\hspace{2em}In this work, we follow the method in \cite{Amberg2007} which is implemented for surface graphs. However, due to differences between data used in \cite{Amberg2007} which are surface graphs saved in either points cloud or mesh format, and our data which are pathways in streamlines format and saved in \textit{ply} file as shown in figure \{\ref{fig:data}\}, we had to build a tool for reading the \textit{ply} file format as streamlines and build graphs out of it. In this case, each \textit{ply} file has one pathway.

\begin{figure}[h!]
\centering
\includegraphics[scale=0.5]{002_data}
\captionsetup{justification=centering}
\caption{A sample of a brain pathway saved in a \textit{ply} file.}
\label{fig:data}
\end{figure}

\subsection{The PLY file}
\hspace{2em}Let us illustrate a bit of our data and the \textit{ply} files we used to save our data, as shown in figure \{\ref{fig:data}\}. The \textit{ply} file consists of two parts: the header and body. The header consists of all information that is necessary to parse the file, and the body has the data itself.
\begin{itemize}
\item The first line represents the file type
\item The second line is the file format
\item The third line is a comment
\item The fourth line contains the name and corresponding number of the first group of elements (each ply file can have any number of elements). Specifically, we can see that there are 69283 vertices in the file.
\item The 5th, 6th and 7th lines shows that the first group of elements has three properties, each of them in float format as well as  the name of each property.
\item The 8th line shows the second group of elements which has 603 fiber tracts.
\item The 10th line shows the end of the header part.
\item The remaining lines are the body of the file which has all the data, with each element in one line.
\end{itemize}

\hspace{2em}Before we start registering the graphs, it is better to have the initial position which makes the template graph $S$ as close as possible to the target graph $T$ and in the best possible alignment before ICP. To do so, we use Principal Components Analysis which is illustrated below.

\section{Principal Components Analysis }
\hspace{2em}Before applying PCA, we scale the template graph $S$ and the target graph $T$ to $[0,1]$. This is done by subtracting all points from the minimum value in the graph and then dividing them by the maximum value in the graph. Next, we apply PCA (from \textit{sklearn.decomposition.PCA}) and use $[-1,1]^3$ cube combination and measure the Euclidean distance between each point in the template graph to the closest point in the target graph as illustrated in the equation $\sum ||v_i-v_j||_F^2$ where $v_i \in S, v_j \in T$. This formula is applied eight times to select the combination with the minimum distance as illustrated in table \{\ref{table:cube}\} and in figure \{\ref{fig:pca}\}. Because we have to keep the target graph $T$ fixed, we apply the transformation calculated from both graphs on template graph $T$, thus the transformation of target graph $T$ has to apply oppositely.
\vspace{2em}
\begin{center}
\begin{table}[h]
	\begin{tabular}{| c | c | c | c | c | c | c | c |}
	\hline
	1 & 2 & 3 & 4 & 5 & 6 & 7 & 8\\
	\hline
	(x,y,z) & (x,y,-z) & (x,-y,z) & (x,-y,-z) & (-x,-y,z) & (-x,y,-z) & (-x,y,z) & (-x,-y,-z)\\
	\hline
	\end{tabular}
\caption{$[-1,1]^3$ cube combination}
\label{table:cube}
\end{table}
\end{center}

\begin{figure}[h!]
	\centering
	\begin{subfigure}[b]{0.59\textwidth}
	\includegraphics[width=\textwidth]{003_original}
	\caption{Original position}
	\end{subfigure}
	% separate
	\begin{subfigure}[b]{0.40\textwidth}
	\includegraphics[width=\textwidth]{004_pca}
	\caption{After PCA}
	\end{subfigure}
\caption{Applying PCA for initial alignment before ICP}
\label{fig:pca}
\end{figure}

%\pagebreak

%\vspace{2em}
\section{The Cost Function}
\hspace{2em}The simplest form of the cost function is shown in the equation (\ref{equ:equation1}):

\begin{equation}
\label{equ:equation1}
||Ax-b||^2
\end{equation}

This simple form of the cost function makes it easy to solve by the \textit{LSQR} (\textit{scipy.sparse.linalg.lsqr}) algorithm which is the implementation of a conjugate-gradient type method for solving sparse linear equations and sparse least-squares problems \cite{Paige1982a}.

To illustrate the cost function more we need to divide it into three parts, as shown in equation (\ref{equ:costFun1}). These include the distance term, stiffness term, and landmark term.

\begin{equation}
E(X) = E_{d}(X) + \alpha E_{s}(X) + \beta E_{t}(X)
\label{equ:costFun1}
\end{equation}

\subsection{Distance Term}
\hspace{2em}The first part of the cost function (\ref{equ:costFun1}) is the distance term, it represent the summation of distances between each point in the template graph $S$ and its correspondent point in the target graph $T$, as illustrated in equation (\ref{equ:distance1}):

\begin{equation}
E_{d}(X) = \sum_{v_{i} \in V} w_{i}dist^2(T,X_{i}v_{i})
\label{equ:distance1}
\end{equation}

where $W = [w_{1}, w_{2}, w_{3}, ..., w_{n}];\quad w_{i}\in [0,1]$ is weight which is \textit{one} if the correspondent point distance is below the threshold and \textit{zero} if not, even some points in template graph do not have correspondences in target graphs $T$, they move with their neighbors due to stiffness term. $T$ is the target graph, $X_{i}$ is the correspondent affine matrix in size $3\times4$, $v_{i}\in V$ is template graph $S$ vertex in homogeneous coordinates $v_{i} = [x,y,z,1]$ because we want to include transformation on affine matrix $X_{i}$ and the distance between each point in the template graph $S$ and its closest point in target graph $T$ is represented by $dist^2(T,X_{i}v_{i})$.

To get the distance, $dist^2(T,X_{i}v_{i})$ become as below:

\begin{equation}
dist^2(T,X_{i}v_{i}) = ||X_{i}v_{i}-u_{i}||^2
\label{equ:distance2}
\end{equation}\\

We calculate the result of $||X_{i}v_{i}-u_{i}||^2$ while the algorithm reduces the distance and improves the alignment iteratively.

If we look back to equation (\ref{equ:distance1}) and combine it with equation (\ref{equ:distance2}), then we have a complete equation (\ref{equ:distance3})

\begin{equation}
\bar{E}_{d}(X) = \sum_{v_{i}\in V} w_{i}||X_{i}v_{i}-u_{i}||^2
\label{equ:distance3}
\end{equation}
\begin{equation*}
= \left\|(W \otimes I_{3}) \left(
\begin{bmatrix}
X_{1} & & \\
& \ddots & \\
& & X_{n}
\end{bmatrix}
\begin{bmatrix}
v_{1} \\ \vdots \\ v_{n}
\end{bmatrix} -
\begin{bmatrix}
u_{1}\\ \vdots \\ u_{n}
\end{bmatrix}
\right) \right\|^2
\end{equation*}\\

where $W = diag(w_{1},\dots, w_{n})$ corresponds to the weights in the diagonal matrix multiplied by the Kronecker product $\otimes$ to $I_{3}$ identity matrix. $X$ is a diagonal matrix of $X_{i}$ that we need to solve and is multiplied by vertices matrix $V=[v_{1},v_{2},v_{3}, \dots,v_{n}]^T$ of template graph $S$, subtracted by the corresponding vertices matrix $U=[u_{1},u_{2},u_{3}, \dots,u_{n}]$ of the target graph $T$.

Let us clarify how \textbf{Kronecker product} works, it is operation of two matrices of arbitrary size denoted by $\otimes$. It is a generalization of the outer product, (denoted by the same symbol) from vectors to matrices, and gives the matrix of the tensor product with respect to a standard choice of basis \cite{Wikipedia2019}.

For instance, If $A$ is an $m \times n$ matrix and $B$ is a $p \times q$ matrix, then the Kronecker product $A \otimes B$ is the $mp \times nq$ block matrix:

\begin{equation*}
A \otimes B =
\begin{bmatrix}
a_{11}B & \dots  & a_{1n}B \\
\vdots  & \ddots & \vdots  \\
a_{m1}B & \dots  & a_{mn}B
\end{bmatrix}
\end{equation*}\\

Thus, the result of $W \otimes I_{3}$ is a $3n\times 3n$ matrix, where matrix $X$ has size $3n\times4n$, $V$ has a shape $4n\times1$ and $U$ has shape $3n\times1$.

We continue to reform the equation to be easily differentiated and implemented by converting it into canonical form. First we swap the positions of the $X$ and $V$ matrices, then we define a sparse matrix $D$ which contains the vertices $v_{i} \in V$ in diagonal position as demonstrated below:

\begin{equation}
D =
\begin{bmatrix}
v_{1}^T & & & \\
& v_{2}^T & & \\
& & \ddots & \\
& & & v_{n}^T
\end{bmatrix}
\label{equ:matD}
\end{equation}\\

The new format of the equation becomes:

\begin{equation}
\bar{E}_{d}(X) = ||W(DX-U)||_{F}^2
\label{equ:distance4}
\end{equation}

\hspace{2em}Where the matrix $W$ has size $n\times n$, the matrix $D$ has size $n\times 4n$ (vertices $v_{i}$ are in homogeneous coordinates $[x,y,z,1]^T$), $X$ has size $4n\times 3$, and $U$ does not change with shape $n\times 3$ (vertices $u_{i}$ are in 3D coordinate).

\subsection{Stiffness Term}
\hspace{2em}The stiffness term is the second part of the equation (\ref{equ:costFun1}). It regularizes the deformation of template graph $S$ by penalizing the weighted difference of the transformations of neighboring vertices under the Frobenius norm $||.||_{F}$ using a weight matrix $G := diag(1, 1, 1, \gamma)$ \cite{Amberg2007}, that keeps the distance between vertices which are neighbours (i.e., vertices which have an edge between them) and also in the same tract constant.
\begin{equation}
E_{s}(X) = \sum_{i,j \in E} ||(X_{i} - X_{j})G||_{F}^2
\label{equ:stiffness1}
\end{equation}

The parameter $\gamma$ is used to balance the rotation and skew against translation while transforming the template graph $S$ \cite{Amberg2007}. In our case $\gamma$ is \textit{one} as we have already scaled our data into the $[-1, 1]^3$ cube.

To make our function solvable directly we need to write it as a quadratic function. Therefore we use twelve parameters per vertex in $3 \times 4$ shape and an $\alpha$ parameter in equation (\ref{equ:costFun1}) which is a constant that manages the contribution of the stiffness term; when it is high, the neighbor vertices do not move far away from each other and when it is low, a greater deformation can occur.
The part $||.||_{F}$ is the \textit{Frobenius norm} which is illustrated in the example below \cite{Amberg2007}:

\begin{equation*}
||A||_{F} = \sqrt{\sum_{i=1}^m \sum_{j=1}^n |a_{ij}|^2}
\end{equation*}

To make the stiffness term more simpler and applicable, we put it in the form below:

\begin{equation}
E_{s}(X) = ||(M\otimes G)X||_{F}^2
\end{equation}

where $M$ is a node-arc incidence matrix defined for directed graphs. The number of rows in this matrix equal the number of nodes (vertices) and the number of columns equal the number of arcs (edges) on the graph and the values must be \textit{zeros, ones and/or minus ones}. The value will be \textit{zero} if the edge on this column is not connected to the vertex on this row where the value lies, otherwise, the value must be either $1$ or $-1$; it will be $1$ if the edge direction is coming towards the vertex and $-1$ otherwise. As illustrated in the figure \{\ref{fig:directed_graph}\}, the matrix $M$ has size $e\times n$ where \textit{e} and \textit{n} are the number of edges and vertices respectively. $G = diag(1,1,1,\lambda)$ is the diagonal matrix and the result of $M \otimes G$ is $4e \times 4n$ matrix.


\begin{figure}[h!]
\centering
\includegraphics[scale=0.2]{006_direct_graph}
\captionsetup{justification=centering}
\caption{Oriented graph with corresponding incidence matrix  \cite{Wikipedia2010}}
\label{fig:directed_graph}
\end{figure}

\subsection{The Landmark Term}
\hspace{2em}The landmark term gives the initial position of the template graph $S$ shown in equation (\ref{equ:landmark1}) below:

\begin{equation}
E_{l}(X) = \sum_{(vi,l) \in L}||(X_{i}v_{i} - l)||^2
\label{equ:landmark1}
\end{equation}\\

Where $L = \left\{(v_{i1},l_{1}),(v_{i2},l_{2}),...,(v_{il},l_{l})\right\}$ is the set of landmarks that maps the template graph $S$ to the target graph $T$. The $\beta$ parameter in equation (\ref{equ:costFun1}) controls the influence of the landmark term.

\subsection{The Final Cost Function}
After simplifying each term separately, we are going to join them together in one equation, (\ref{equ:costFun2}) as outlined below:

\begin{equation}
\bar{E}(X) = \left\|
\begin{bmatrix}
\alpha M \otimes G \\ WD \\ \beta D_{L}
\end{bmatrix}
X -
\begin{bmatrix}
0 \\ WU \\ U_{L}
\end{bmatrix}
\right\| ^2
\label{equ:costFun2}
\end{equation}\\

The current form of the function can be minimized by setting its derivative to zero and solving it as a linear equation. The minimum value of $\bar{E}(X)$ is when $X = (A^T A)^{-1} A^T B$.

Even $A$ a sparse matrix and most of its values are zeros, however, it is still a large matrix and requires the most time to solve the equation. In order to solve the equation at a faster speed, we omitted the landmark term because we already have the initial position by applying PCA. Once we omit the landmark term, the function will be:

\begin{equation}
E(X) = \left\|
\begin{bmatrix}
\alpha M \otimes G \\ WD
\end{bmatrix}
X -
\begin{bmatrix}
0 \\ WU
\end{bmatrix}
\right\| ^2
= ||Ax - b||_{F}^2
\label{equ:costFun3}
\end{equation}\\

\subsection{Implementing Cost Function}
\hspace{2em}To implement the equation (\ref{equ:costFun3}), we use the sparse matrix from \textit{scipy.sparse} to build $A$. First we measure the euclidean distance between each point $v_{i}\in V$ in the template graph $S$ and its correspondent point $u_{i}\in U$ in the target graph $T$ using \textit{K-D Tree} from \textit{sklearn.neighbors.KDTree}, then we filter distances by a threshold value (which is a parameter whose criteria for selection will be discussed later ) to define \textit{zeros} and \textit{ones} and put them on $n\times n$ diagonal sparse matrix \textit{W} (\textit{scipy.sparse.diags}).

\begin{equation*}
W =
\begin{bmatrix}
w_{1}^T & & & \\
& w_{2}^T & & \\
& & \ddots & \\
& & & w_{n}^T
\end{bmatrix}
\end{equation*}\\

Next we build a $n\times 4n$ sparse matrix \textit{D}. First we join all tracts on a template graph $S$ to get points cloud matrix $V$ which is a $n\times 3$ matrix and we add one more column at the end to have homogeneous coordinates $V=[v_{1}, v_{2}, v_{3}, \dots , v_{n}]^T$. Then we take each vertex $v_{i} \in V$ and place it diagonally in the sparse matrix \textit{D},  as shown in equation (\ref{equ:matD}). To get \textit{WD} as in equation (\ref{equ:costFun3}), we calculate the dot product of \textit{W} and \textit{D} using the function \textit{scipy.sparse.dot} in the sparse matrix \textit{W} ($W.dot(D)$). The result is a $n\times 4n$ matrix. To get \textit{WU}, we use the same dot product function and the result is a $n\times 3$ matrix.

As we can see in equation (\ref{equ:costFun3}), the stiffness term is only for the template graph $S$. To implement the stiffness term, we create a diagonal sparse matrix $G = diag(1,1,1,\lambda)$ and $\lambda=1$ in our case. Then we build an $e\times n$ \textit{M} matrix. To do so, we use the streamlines form of template graph $S$ which has the connections of between points in the same tract (streamline). We next calculate \textbf{Kronecker product} of $M \otimes G$. The $\lambda$ parameter is a variable the user can adjust depending on the data.

To have the final form of the cost function, we vertically stack $\alpha M \otimes G$ on top of $WD$ in one sparse matrix $A$ with size $n\times 4n$.  The sparse matrix $A$ is a part of the equation (\ref{equ:costFun3})

As previously discussed, we already have \textit{WU}; To get the sparse matrix $b$ in the equation (\ref{equ:costFun3}), we create a sparse matrix of zeros with size $n\times 3$ and vertically stack it on top of \textit{WU}.

The cost function $||Ax-b||_{F}^2$ can then be solved by using \textit{LSQR} from \textit{scipy.sparse.linalg.lsqr} which takes $A$ and $b$ as parameters and returns $X=[x_{1}, x_{2}, x_{3}, \dots, x_{n}]$. However, as we can observe, $b$ in our case is an $n\times 3$ matrix and \textit{scipy.sparse.linalg.lsqr} takes $b$ as vector only. To solve this, we use the fundamental principle of matrix multiplication by applying \textit{LSQR} for each column in matrix $b$.

To get the aligned template graph $S$, we calculate the dot product of the $x$, returned by \textit{LSQR}, and matrix $D$. The result is a $n\times 3$ matrix which is points cloud form of the template graph $S$ the must be converted to original form (i.e., streamlines).\\

Finally for visualization, we customize functions from \textit{Open3D} library to view brain tract bundles.

\begin{figure}[h!]
\centering
\includegraphics[scale=0.2]{008_ide}
\captionsetup{justification=centering}
\caption{IDE and visualization tool}
\end{figure}

\end{document}