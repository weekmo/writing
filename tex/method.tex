\documentclass[../structure.tex]{subfiles}
%\usepackage{../mypkg}
\begin{document}
\chapter{Method}
The main subject of this master thesis is a computationally eective surface
registration algorithm that produces satisfactory results when faced with
the problem of the registration between meshes or cloud point data with a
large number of points. In this Chapter the idea of this method is described
theoretically. It is based heavily on the method presented in [1] as it is built
as a direct extension of an already existing algorithm. The main dierences
will be addressed here in more detail.
We define a surface registration the process of aligning two 3D surfaces,
a target and a template one, so that they meet when placed at the same
position and orientation. The result should be a solution of the corresponding
problem, namely a mapping that lists the positions of the semantically
corresponding points between the two surfaces. Because this is a dense registration
method, correspondences for all points are looked for, not only for
selected feature points, as in sparse methods. This is done by deforming the
template surface, locally moving it closer and closer to the target, in order
to wrap it onto it.
As described in [1] the template surface is denoted by S = (V,E), where
V is a set of n vertices and E is a set of m edges and the target surface is T.
In the sense of this algorithm the term registration means finding parameters
X that describe a set of displaced vertices V (X). As shown on Figure 3.1,
the template surface S is deformed onto the target surface T by applying the
locally affine transformations Xi. These transformations are guided by the
previously found correspondences, in our case the closest points in the target
for the template points. Those points are denoted by ui for each displaced
vertex Xivi from the template.
In our case, because the transformations Xi are affine and we are using
homogeneous coordinates in the 3D Euclidean space, the parameters Xi are
a 3 4 matrices for each vertex in the template. The unknown Xi transformations
are organized in the following 4n 3 matrix X:
X = [X1, . . . ,Xn]T (3.1)
Having this defined, this transformations need to be regularized, so that
the vertices that are originally close to each other will stay that way after each
step of the deformation has occurred. This is done by defining constraints
and using them in finding the transformations. A cost function is build and
minimized by setting its derivative to zero and solving the resulting equation.
The result is the needed matrix X.
In the subsection Setup we describe how the input data is presented and
the rest of the needed definitions are cleared out. It also covers the topic
of dierent kinds of input data, as the tool can be applied on both triangle
meshes and point cloud data. The the second subsection two preliminary
steps are introduced as well as the nature of their necessity. The subsection
Clustering is about the main part of the optimization, its idea, the stages it
went through and how is it applied. The role that the clustering played in this
thesis and the developing of the tool went from optimization of the solving
part to the main instrument of the regularization. The subsection Assigning
Soft Membership Weights briefly defines the essential topic of the influence
the clusters have on the vertices in their area. The second to last subsection
and its subsections describe how the equation changed due to the optimization
and what is its final look. They also address questions like how the
correspondences are found, how the method works for unconnected graphs,
and how are the found transformations applied on the template graph using
soft membership weights. The last subsection Nonrigid optimal step ICP
algorithm shows how exactly the base algorithm works after the alternation.
\section{Setup}
The method as well as the developed tool will recognize two kinds of input:
triangle mesh data and point cloud data. In both cases the template mesh
needs to be adjusted to the previously defined representation of the input
data, namely a graph S = (V,E), where V and E are sets of vertices and
edges respectively. The only important thing about the representation of the
target mesh T is to allow quick finding of the closest point ui in the target
mesh for any given point of the template mesh. Back to the template surface
representation, the case of the triangle mesh is clear, the edges set will be
constructed based on the triangulation, each triangle will produce 3 edges in
the set with no duplications, meaning that if an edge is already in the set
and a currently observed triangle contains it, it will not be added a second
time.
In the case of the point cloud data a dierent approach is needed, because
there is no available information in the input for the connectivity of the graph.
An efficient algorithm for constructing a K-nearest neighbors graph is used.
The only required input for it is the number k of the nearest neighbors
that are to be found. In the cases when k is too small and the resulted
graph is unconnected the method will still perform without a problem if
some conditions are fulfilled, as it will be explained in further details later
in this Chapter, the subsection Clustering. The same applies for the triangle
mesh input, it can contain unconnected groups of vertices without causing a
problem for the method.
From here on we will address the input data as target and template
graphs, independent of their origin. Another important thing to be noted
about the input graph is that each edge has a weight and in our case this
weight is the Euclidean distance between the two vertices that are connected
with that edge. Usually this information doesn’t come with the triangle
meshes or the point cloud, so it is computed after the scaling step. These
weights are also updated after each transformation step, because the vertices
change their position and the Euclidean distance between them also varies.
This will be addressed again in the subsection Nonrigid optimal step ICP
algorithm, where each step will be noted again in a sequence.
\section{Preparation Steps}
Two preparations steps are done on both the template and the target graphs
before any clustering or building of equations has started: principal components
analysis and scaling to the [-1, 1]3 cube. The first is necessary so that
the two graphs are aligned as much as possible with one another, which will
contribute to finding adequate corresponding points to the template vertices
in the target graph, as shown in Figure 3.2.
The tutorial cited in [36] was used for the purpose of applying the Principal
component analysis (PCA) to the target graph and after this is done to
apply it the template as well, but according to the orientation of the already
aligned target. The steps are the following:
1. Collect the vertices of the graph and their coordinates in a matrix Data,
where each vertex is on a row and its coordinates are on the columns.
Because we are considering 3D data this matrix will have the size of
|V |3.
2. Subtract the mean (X, Y and Z accordingly) from each data dimension
to get a new matrix DataAdjust.
3. Calculate the covariance matrix
C =
2
4
cov(x, x) cov(x, y) cov(x, z)
cov(y, x) cov(y, y) cov(y, z)
cov(z, x) cov(z, y) cov(z, z)
3
5
where
cov(X, Y ) =
Pn
i=1
(Xi - X)(Yi - Y )
(n - 1)
4. Calculate the eigenvectors and eigenvalues of the covariance matrix.
5. Build a feature vector (a matrix of eigenvectors with size of 3  3)
according to a chosen orientation
FeatureV ector = (eig1 eig2 eig3)
where the eigenvectors are ordered by their eigenvalues from highest to
lowest and depending on the signs of the coordinates of the eigenvectors
there are 8 orientations available:
1 (x1, y1, z1) (x2, y2, z2) (x3, y3, z3)
2 (-x1, y1, z1) (-x2, y2, z2) (-x3, y3, z3)
3 (x1, -y1, z1) (x2, -y2, z2) (x3, -y3, z3)
4 (x1, y1, -z1) (x2, y2, -z2) (x3, y3, -z3)
5 (-x1, -y1, z1) (-x2, -y2, z2) (-x3, -y3, z3)
6 (-x1, y1, -z1) (-x2, y2, -z2) (-x3, y3, -z3)
7 (x1, -y1, -z1) (x2, -y2, -z2) (x3, -y3, -z3)
8 (-x1, -y1, -z1) (-x2, -y2, -z2) (-x3, -y3, -z3)
6. Compute the final data to work with
FinalData = FeatureV ectorT  DataAdjustT
This gives us our old data in a matrix 3|V |, but oriented regarding
its eigenvectors instead of the usual axes
When those steps are completed for the target graph, they are also run
on the template graph 8 times for each possible orientation and it is checked
which one gives the closest alignment of the template graph to the target one.
Once the right orientation is known, the PCA is also run on the template and
the scaling step can begin. It is as simple as finding the maximum absolute
value of a coordinate of a vertex in both the target and the template graph
and then dividing all the coordinates by this maximum.
It is explained in [1] that the scaling of the data to the [-1, 1]3 cube is
done in order to simplify the stiness part of the equation. While this will
be discussed also in the subsection about the The Original Cost Function,
it is enough to say here that if the parameter ! is used to weight the differences
between the rotational and skew parts of the deformation against
the translational part, and it is determined based on the graph’s units, then
there will be no need of taking it into consideration when the graph is scaled
in advance and ! will be set to 1.
After the graphs are aligned and scaled the registration can begin. One
of the biggest parts in it is the clustering of the data. Why it is done and
how is explained in the next subsection of this Chapter.
\section{Finding The Affine Transformation}
Now that we have the cluster graph and we want to perform a registration on
it, it is time to go back to [1] and look more into the details of constructing
the cost function, the solving of the equation that works as regularization
and how to use the result for the registration, all in the current case of the
clustering. This subsection is divided in three, the first part explains how
the things are done in [1], the second part shows how the cost function diers
from the original when using clusters and the third part covers the topic of
the transformation of the template graph once the parametrization of the
mapping (3.1) is found.
\subsection{Cost Function}
In this subsection the template graph is the original one and no talks of
clustering will be taking place. All of the optimization topics will be covered
in the next subsection, while the goal here is to show the original cost function
and the way it is defined in [1]. Also in this subsection we say that n = |V | and e = |E| for simplicity.
This cost function consists of three terms: distance term, stiness term
and landmark term. Those terms express dierent aspects of what we should
achieve when minimizing the cost function. The distance term describes that
the distance between the transformed template graph and the target graph
needs to be small. The stiness term is used for the regularization, so that
neighbor vertices move together and there won’t be any huge deformations
that contradict with the overall connectivity. The landmark term is used for
initialization, assigning starting correspondences between the two graphs.
Lets look all those terms in detail.
In [1] the cost function is defined in the following way:
%E(X) = Ed(X) + Es(X) + #El(X) (3.6)
and this is the way it is defined in this thesis as well with few dierences that
will be pointed out. The first term is the distance term and it is defined in
the following way:
Ed(X) =
X
vi2V
widist2(T,Xivi) (3.7)
In this representation the template vertices are noted with vi and are also
given in homogeneous coordinates vi = [x, y, z, 1]T . This is also the reason
why Xi are 3 4 matrices in (3.1).
The distance between the point in the template v and it closest point
in the target is denoted as dist(T, v) and wi is a weight that is one, if a
correspondence for this vertex is found in the target, and zero if not. The
correspondences are used so that each vertex in the template moves in the
direction of the vertex in the target that corresponds to it. Some vertices
have no correspondences, but even then they move along their neighbors
because of the stiness term. In [1] is said that in order to detect which
vertices have no correspondence three tests are used, defined in the following
way: A correspondence (Xivi, ui) is dropped if
1. ui lies on a border of the target mesh,
2. the angle between the normals of the meshes at Xivi and ui is larger
than a fixed threshold or
3. the line segment Xivi to ui intersects the deformed template.
In our implementation we simplify things and say that a correspondence
is not found if the distance between the point and its closest target correspondence
is greater than a value that is set in the beginning as a maximum
correspondence distance. As noted before, it is important for the representation
of the target graph to be convenient enough for an efficient search for
correspondences. It will be explained in the Chapter Implementation how
this can be approached.
The second term, the stiness term that has the job to regularize the
deformation, is defined in the following way in [1]:
Es(X) =
X
i,j2E
k(Xi - Xj)Gk2
F (3.8)
where G is the diagonal matrix diag(1, 1, 1,!). As mentioned before, ! is set
to one, because the data was scaled into the [-1, 1]3 cube, which is described
in the Preparation Steps subsection. The kkF is the Frobenius norm defined
for a matrix A with size m  n like this:
kAkF =
vuut
Xm
i=1
Xn
j=1 |aij |2
With this term the dierence between the transformations of two vertices
with an edge between them is minimized, keeping neighbor vertices together
as discussed. The stiness weight  that is also a part of (3.6) changes on
each step of the ICP algorithm and controls the transformation. It starts
with a high value that keeps big chunks of the mesh together so that more
global transformations can occur, and each step it gets lower value, so that
the small details can fall into place as well. The starting value, the step with
which it change each step and the minimum of the  is user defined and
it depends on those parameters how many solver steps the algorithm will
produce and how accurate the end result will be.
As explained in [1] and also as will be noted again in the Applying the
Transformations section, twelve parameters are used to describe the displacement
of the vertex, which are the matrices Xi with size 3  4 in (3.8). This
is done in order to construct the cost function as a quadratic function that
can be minimized directly.
The last term is the landmark term:
El(X) =
X
(vi,l)2L
k(Xivi - l)k2 (3.9)
where L is a set of given landmarks (vi1, l1), . . . , (vil, ll). A landmark (vi, l)
is a mapping from the template graph to the target one and L has a role to
% help the registration start with initial mapping. The landmark weight # in
(3.6) is used to control this landmark term, having a strong influence in the
first steps of the ICP algorithm and fading gradually, so that any noise from
the initial landmarks is avoided. In this implementation the landmark term
is omitted, because the registration can very well give good results without
it. Also after the PCA is done as a preparation step and both graphs are
aligned the need for landmarks is even less notable.
In order to transform the cost function (3.6) in a way that can be minimized,
we need to assume that the correspondences (vi, ui) are fixed. This
way the part of the distance term (3.7)
dist2(T,Xivi) (3.10)
can become
kXivi - uik2 (3.11)
and the distance term can be rewritten. The entire cost function can be
presented using matrices and it can be solved directly. With the changed
distance term and the the other two terms also rewritten with matrices we
have
¯E
% (X) = ¯Ed(X) + Es(X) + #El(X) (3.12)
that can be minimized enough times so that a local minimum of (3.6) is
found.
Now let’s look at the changed distance term in more detail. When applying
the change from (3.10) to (3.11) on (3.7) we have:
¯E
d(X) =
X
vi2V
wikXivi - uik2 =
=
*******
(W XOR I3)
2
64
X1
. . .
Xn
3
75 2 64
v1
...
vn
3
75
-
2
64
u1
...
un
3
75
!
*******
2
(3.13)
where W is diag(w1, . . . ,wn), the matrix containing the correspondences
weights on its diagonal, I3 is the 3  3 identity matrix and the Kronecker
product is denoted by XOR. By definition the Kronecker product of twomatrices
is the following matrix:
A XOR B =
2
64
a11B .. .a1nB
...
. . .
...
am1B .. .amnB
3
75
This is done so that the W XOR I3 part will have size of 3n  3n, while the
rest of the matrices have sizes of 3n4n, 4n1 and 3n1 accordingly, the
weight wi will still influence the Xivi - ui part and the weights can be put
inside of the norm.
The equation (3.13) still needs to be rearranged into its canonical form
in order to be clearly visible what the unknowns are and for them to be
arranged in a compact matrix X as defined in the beginning in (3.1). This
is done by swapping the positions of the transformation matrices Xi and the
vertices vi and by defining the following matrix D:
D =
2
6664
vT
1
vT
2
. . .
vT
n
3
7775
(3.14)
In the end we get the final simplified form of the distance term:
¯E
d(X) = kW(DX - U)k2
F (3.15)
where the matrix W has size nn, thematrix D had size n4n, because the
vertices are presented with homogeneous coordinates, as it was noted earlier,
and the matrix X is as before with size 4n3. The matrix U still holds the
correspondences ui and is with size n  3.
Following similar simplifications as the distance term and the new notations,
we get the stiness term (3.8) in its new matrix form:
Es(X) = k(M XOR G)Xk2
F (3.16)
where M is the node-arc incidence matrix for the template graph. It is
defined so that if edge r connects the vertices (i, j) the nonzero entries of M
in row r are Mri = -1 and Mrj = 1 as shown in Figure 3.6. It is important
to have in mind here that the edges and vertices of the mesh are numbered
and its edges are directed from the lower numbered vertex to the higher
numbered.
The matrix M has size e  n and so M XOR G has size of 4e  4n.
At last the landmark (3.9) term also gets its makeover:
El(X) = kDLDX - ULk2
F (3.17)
where DL is the matrix D from (3.14), but with only those vertices that
are landmarks, and the matrix UL consists of the corresponding vertices of
those landmarks in the target, so UL = [l1, . . . , ll]T .
The final cost function (3.12) will look like this:
¯E
(X) =
******
2
4
M XOR G
WD
% #DL
3
5X -
2
4
0
WU
UL
3
5
******
2
= kAX - Bk2
F (3.18)
Using that ¯E(X) takes on its minimum at X = (ATA)-1ATB we solve
the resulting system in each step of the inner loop of the algorithm. Because
the goal of this registration is to be used on meshes with large number of
points, the matrix A tends to get large, and even though it is sparse and most
of it is full of zeros, it causes the solving step to be the most time consuming
part of the whole algorithm. This is why we discuss its optimization in the
next subsection.
\subsection{Optimizing the Cost Function}
As it was mentioned before, the clustering step described in the subsection
Clustering creates a cluster graph that has the medoids of the clustered
template graph as vertices, and edges connecting them as the template graph
is. The goal of the optimization is to use the cluster graph instead of the
original template one and still get good results. This also changes the way the
algorithm steps are defined. While in [1]  goes from a user defined value to
another, for example from 100 to 1 with a step 1, in the case of clustering the
algorithm stops when the template cannot be divided into smaller clusters.
This will be further explained in the Nonrigid optimal step ICP algorithm
subsection.
So lets look again at (3.18) and discuss in this subsection how it diers
in the case of the cluster optimization. We can even consider the situation
when the landmark term is omitted. In this case the cost function looks like
this:
¯E
(X) =
****

M XOR G
WD
0
X -

0
WU
0****
2
(3.19)
and so the matrices A and B that will be addressed couple of times in
this subsection are accordingly:
A =

M XOR G
WD
0
(3.20)
B =

0
WU
0
(3.21)
The stiness weight  is one of the things that change in the clustering
optimization. It is in fact removed entirely, because of the way the cluster
graph was constructed. All its edges have their own weight as discussed and
its substance is to say how strong the two medoids connected by an edge
are holding together. This is done by counting how many edges there are
between the vertices of the two clusters and this number is the weight of the
edge between the medoids that act as vertices in the cluster graph. Those
weights start big enough, because in the beginning the clusters are fewer and
therefore bigger, that brings a lot of edges adding their numbers to the sum.
This is the alternative to the original algorithm starting with a large . Along
the run of the algorithm the clusters are divided into smaller chunks until
they reach a minimum size that is defined by the user, and so the weights
grow smaller. Again this bring the same results as the decreasing of the 
value.
The clusters act like the stiness weight also because all the vertices that
belong to a cluster are transformed in a way that depends on the medoid
of this cluster and its neighbor medoids as was mentioned in the Assigning
Soft Membership Weights section and will be explained again in Applying the
Transformations subsection. At the same time omitting entirely the stiness
term gave more unsatisfying results in the registration part. And yet even
with the part of the matrix A, that accounts for the connectivity of the graph,
still present, it is small enough through the large part of the algorithm run,
because the cluster graph is much smaller than the original template graph
even when the clusters get divided in time.
Next optimization aects the part of matrix A that is the distance term,
the matrix multiplication WD, and also WU that is a part of matrix B also
belonging to the distance term. In this subsection again it is defined that the
n = |V | and e = |E| for the original template graph, and also ¯n = | ¯ V | and
¯e = |¯E | for the cluster graph. The new matrix ¯D unlike in (3.14) is defined
the following way: Each row corresponds to a vertex in the original graph
and every four columns correspond to a cluster. If a vertex vi belongs to a
cluster Cj then the coordinates of the vertex are set in the row i and on the
4 columns for the cluster 4 <- j, 4 <- j +1, 4 <- j +2 and 4 <- j +3. More formal
an element | ¯ dij | in matrix ¯D is defines in the following way:
| ¯ dij | =
(
vi if vi 2 Cj
0 otherwise
(3.22)
Defined like that the new matrix ¯D has size n  4¯n.
The algorithm works for unconnected graphs, as it was mentioned in the
subsection Clustering. The reason for this is the modification of the matrix
D that was just described. Because of this, even if there is a unconnected
cluster, in the case of the cluster graph - unconnected vertex, or many, the
matrix will not have a column entirely constructed of zeros. This would
otherwise prevent the solving of the equation, making it underdetermined.
But with a matrix ¯D the matrix A has a determinant dierent than zero
and the solving step can occur. Unfortunately there are exceptions of this,
for example when the unconnected component is constructed of too small
number of vertices and no correspondences for them are found. Then the
system is underdetermined and cannot be solved.
Even a better optimization in the sense of better results can be made
if the matrix ¯D is modified little more. For this the membership weights
from Assigning Soft Membership Weights section are used. We say that the
transformation of a vertex depends on the medoid of the cluster it belongs to,
but also on the clusters that are neighbors of this cluster and soft membership
weights for each vertex and its clusters are assigned using (3.5). Having
this, another version of matrix D is denoted with ˆD and constructed in the
following way: Again each row corresponds to a vertex in the original graph
and every four columns correspond to a cluster. The coordinates of the vertex
vi are set in the row i and on the 4 columns of the clusters Cj that the vertex
”belongs” to, multiplied by the membership weight for the vertex and the
according cluster. A vertex ”belongs” to a cluster if it is in a cluster or if the
cluster is a neighbor of the cluster the vertex is in. Again more formal this
means the following:
| ˆ dij | =
(
mvi
j vi if Cj 2 Cvi
0 otherwise
(3.23)
where the membership weight is defined as in (3.5) as well as the cluster
set Cvi. The matrix ˆD still has the same size as ¯D: n  4¯n and even though
fewer elements in it are zeros and the sparse optimization is less notable, the
results in the registration are better.
Because of those modifications the matrix W contains the corresponding
weights for the original template vertices, not the cluster graph’s ones. The
same goes for the matrix U in the matrix B, where the correspondences ui
are n and not ¯n.
If we include the landmark term, only the DL matrix needs to be modified
in a similar fashion as the matrix D, keeping the vertices that have landmarks
in their own rows, but distributed in their corresponding clusters’ columns,
% and the landmark weight # needs to stay as it is.
After those optimizations the matrix A as written in (3.20) has size of
(4¯e+n)4¯n and the matrix B as written in (3.21) has size of (4¯e+n)3. But
as the main equation was defined in (3.2), for the solving we need the matrices
ATA and ATB and those have sizes accordingly 4¯n4¯n and 4¯n3 which is
really small in the beginning, because the clusters are a small number, and
even later in the algorithm run, it doesn’t get too big if the minimum number
of vertices in a cluster is set to something reasonable. Thus we get better
computation time.
In the next subsection the applying of the resulting transformation matrix
X is discussed and
\end{document}